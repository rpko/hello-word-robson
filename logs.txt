
==> Audit <==
|---------|----------------------------|----------|--------|---------|---------------------|---------------------|
| Command |            Args            | Profile  |  User  | Version |     Start Time      |      End Time       |
|---------|----------------------------|----------|--------|---------|---------------------|---------------------|
| start   |                            | minikube | robson | v1.34.0 | 21 Nov 24 17:49 -03 |                     |
| start   | --driver=docker            | minikube | robson | v1.34.0 | 21 Nov 24 17:50 -03 |                     |
| start   | --driver=docker            | minikube | robson | v1.34.0 | 21 Nov 24 17:56 -03 | 21 Nov 24 17:57 -03 |
| service | hello-world-robson-service | minikube | robson | v1.34.0 | 21 Nov 24 18:06 -03 |                     |
| service | hello-world-robson-service | minikube | robson | v1.34.0 | 21 Nov 24 18:06 -03 |                     |
|---------|----------------------------|----------|--------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/11/21 17:56:13
Running on machine: NTBROBEPAT863
Binary: Built with gc go1.22.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1121 17:56:13.210898   65670 out.go:345] Setting OutFile to fd 1 ...
I1121 17:56:13.211897   65670 out.go:397] isatty.IsTerminal(1) = true
I1121 17:56:13.211902   65670 out.go:358] Setting ErrFile to fd 2...
I1121 17:56:13.211907   65670 out.go:397] isatty.IsTerminal(2) = true
I1121 17:56:13.212087   65670 root.go:338] Updating PATH: /home/robson/.minikube/bin
W1121 17:56:13.212221   65670 root.go:314] Error reading config file at /home/robson/.minikube/config/config.json: open /home/robson/.minikube/config/config.json: no such file or directory
I1121 17:56:13.212814   65670 out.go:352] Setting JSON to false
I1121 17:56:13.213956   65670 start.go:129] hostinfo: {"hostname":"NTBROBEPAT863","uptime":14257,"bootTime":1732208316,"procs":48,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.167.4-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"e62377c5-4753-495c-9788-47653e75a005"}
I1121 17:56:13.214005   65670 start.go:139] virtualization:  guest
I1121 17:56:13.216787   65670 out.go:177] 😄  minikube v1.34.0 on Ubuntu 22.04 (amd64)
W1121 17:56:13.220896   65670 preload.go:293] Failed to list preload files: open /home/robson/.minikube/cache/preloaded-tarball: no such file or directory
I1121 17:56:13.221071   65670 notify.go:220] Checking for updates...
I1121 17:56:13.221361   65670 driver.go:394] Setting default libvirt URI to qemu:///system
I1121 17:56:13.253936   65670 docker.go:123] docker version: linux-27.3.1:Docker Engine - Community
I1121 17:56:13.254036   65670 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1121 17:56:13.284529   65670 info.go:266] docker info: {ID:a0d55bd8-451e-4077-aa48-d475435224a4 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:40 SystemTime:2024-11-21 17:56:13.271762378 -0300 -03 LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16440803328 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:NTBROBEPAT863 Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:57f17b0a6295a39009d861b89e3b3b87b005ca27 Expected:57f17b0a6295a39009d861b89e3b3b87b005ca27} RuncCommit:{ID:v1.1.14-0-g2c9f560 Expected:v1.1.14-0-g2c9f560} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-buildx: no such file or directory Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx]] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-compose: no such file or directory Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose]] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-debug: no such file or directory Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-dev: no such file or directory Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-extension: no such file or directory Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-feedback: no such file or directory Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-init: no such file or directory Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-sbom: no such file or directory Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-scout: no such file or directory Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout]] Warnings:<nil>}}
I1121 17:56:13.284618   65670 docker.go:318] overlay module found
I1121 17:56:13.286660   65670 out.go:177] ✨  Using the docker driver based on user configuration
I1121 17:56:13.288508   65670 start.go:297] selected driver: docker
I1121 17:56:13.288880   65670 start.go:901] validating driver "docker" against <nil>
I1121 17:56:13.288910   65670 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1121 17:56:13.289014   65670 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1121 17:56:13.319831   65670 info.go:266] docker info: {ID:a0d55bd8-451e-4077-aa48-d475435224a4 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:40 SystemTime:2024-11-21 17:56:13.306437568 -0300 -03 LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16440803328 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:NTBROBEPAT863 Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:57f17b0a6295a39009d861b89e3b3b87b005ca27 Expected:57f17b0a6295a39009d861b89e3b3b87b005ca27} RuncCommit:{ID:v1.1.14-0-g2c9f560 Expected:v1.1.14-0-g2c9f560} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-buildx: no such file or directory Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx]] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-compose: no such file or directory Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose]] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-debug: no such file or directory Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-dev: no such file or directory Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-extension: no such file or directory Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-feedback: no such file or directory Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-init: no such file or directory Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-sbom: no such file or directory Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-scout: no such file or directory Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout]] Warnings:<nil>}}
I1121 17:56:13.319963   65670 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I1121 17:56:13.320600   65670 start_flags.go:393] Using suggested 3900MB memory alloc based on sys=15679MB, container=15679MB
I1121 17:56:13.320736   65670 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I1121 17:56:13.323023   65670 out.go:177] 📌  Using Docker driver with root privileges
I1121 17:56:13.324979   65670 cni.go:84] Creating CNI manager for ""
I1121 17:56:13.324999   65670 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1121 17:56:13.325005   65670 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1121 17:56:13.325150   65670 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/robson:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1121 17:56:13.327057   65670 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I1121 17:56:13.329960   65670 cache.go:121] Beginning downloading kic base image for docker with docker
I1121 17:56:13.331896   65670 out.go:177] 🚜  Pulling base image v0.0.45 ...
I1121 17:56:13.333953   65670 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1121 17:56:13.334059   65670 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1121 17:56:13.352035   65670 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1121 17:56:13.352274   65670 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1121 17:56:13.353297   65670 image.go:148] Writing gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1121 17:56:13.641981   65670 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.31.0/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1121 17:56:13.641997   65670 cache.go:56] Caching tarball of preloaded images
I1121 17:56:13.642183   65670 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1121 17:56:13.644753   65670 out.go:177] 💾  Downloading Kubernetes v1.31.0 preload ...
I1121 17:56:13.647126   65670 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 ...
I1121 17:56:13.955335   65670 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.31.0/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4?checksum=md5:2dd98f97b896d7a4f012ee403b477cc8 -> /home/robson/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1121 17:56:26.444428   65670 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 ...
I1121 17:56:26.444505   65670 preload.go:254] verifying checksum of /home/robson/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 ...
I1121 17:56:27.133180   65670 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1121 17:56:27.133493   65670 profile.go:143] Saving config to /home/robson/.minikube/profiles/minikube/config.json ...
I1121 17:56:27.133524   65670 lock.go:35] WriteFile acquiring /home/robson/.minikube/profiles/minikube/config.json: {Name:mkd541a12869ff57d6c755ed99080cb110d14bff Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:56:34.353607   65670 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1121 17:56:34.353614   65670 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1121 17:57:10.631654   65670 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1121 17:57:10.631700   65670 cache.go:194] Successfully downloaded all kic artifacts
I1121 17:57:10.631731   65670 start.go:360] acquireMachinesLock for minikube: {Name:mk4338cd1a905985c6fe5ebb76d1640f1f10a3b7 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1121 17:57:10.631862   65670 start.go:364] duration metric: took 115.691µs to acquireMachinesLock for "minikube"
I1121 17:57:10.632421   65670 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/robson:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1121 17:57:10.632493   65670 start.go:125] createHost starting for "" (driver="docker")
I1121 17:57:10.634697   65670 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=3900MB) ...
I1121 17:57:10.635088   65670 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1121 17:57:10.635171   65670 client.go:168] LocalClient.Create starting
I1121 17:57:10.635977   65670 main.go:141] libmachine: Creating CA: /home/robson/.minikube/certs/ca.pem
I1121 17:57:10.693208   65670 main.go:141] libmachine: Creating client certificate: /home/robson/.minikube/certs/cert.pem
I1121 17:57:10.800659   65670 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1121 17:57:10.818283   65670 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1121 17:57:10.818366   65670 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1121 17:57:10.818383   65670 cli_runner.go:164] Run: docker network inspect minikube
W1121 17:57:10.836442   65670 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1121 17:57:10.836460   65670 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1121 17:57:10.836469   65670 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1121 17:57:10.836548   65670 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1121 17:57:10.858352   65670 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0019922a0}
I1121 17:57:10.858429   65670 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1121 17:57:10.858478   65670 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1121 17:57:11.095830   65670 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1121 17:57:11.095857   65670 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1121 17:57:11.095948   65670 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1121 17:57:11.114643   65670 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1121 17:57:11.135224   65670 oci.go:103] Successfully created a docker volume minikube
I1121 17:57:11.135331   65670 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib
I1121 17:57:13.073456   65670 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib: (1.941791427s)
I1121 17:57:13.073474   65670 oci.go:107] Successfully prepared a docker volume minikube
I1121 17:57:13.073514   65670 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1121 17:57:13.073531   65670 kic.go:194] Starting extracting preloaded images to volume ...
I1121 17:57:13.073582   65670 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/robson/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -I lz4 -xf /preloaded.tar -C /extractDir
I1121 17:57:18.332904   65670 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/robson/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -I lz4 -xf /preloaded.tar -C /extractDir: (5.255705084s)
I1121 17:57:18.332965   65670 kic.go:203] duration metric: took 5.255909754s to extract preloaded images to volume ...
W1121 17:57:18.333404   65670 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I1121 17:57:18.333527   65670 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1121 17:57:18.362357   65670 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3900mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85
I1121 17:57:18.857917   65670 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1121 17:57:18.880525   65670 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1121 17:57:18.906495   65670 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1121 17:57:18.976050   65670 oci.go:144] the created container "minikube" has a running status.
I1121 17:57:18.976076   65670 kic.go:225] Creating ssh key for kic: /home/robson/.minikube/machines/minikube/id_rsa...
I1121 17:57:19.197194   65670 kic_runner.go:191] docker (temp): /home/robson/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1121 17:57:19.227674   65670 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1121 17:57:19.246321   65670 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1121 17:57:19.246332   65670 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1121 17:57:19.306523   65670 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1121 17:57:19.335192   65670 machine.go:93] provisionDockerMachine start ...
I1121 17:57:19.335379   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:19.363060   65670 main.go:141] libmachine: Using SSH client type: native
I1121 17:57:19.363422   65670 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1121 17:57:19.363434   65670 main.go:141] libmachine: About to run SSH command:
hostname
I1121 17:57:19.364520   65670 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:56286->127.0.0.1:32768: read: connection reset by peer
I1121 17:57:22.495485   65670 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1121 17:57:22.495504   65670 ubuntu.go:169] provisioning hostname "minikube"
I1121 17:57:22.495626   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:22.519806   65670 main.go:141] libmachine: Using SSH client type: native
I1121 17:57:22.519956   65670 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1121 17:57:22.519961   65670 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1121 17:57:22.670770   65670 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1121 17:57:22.670832   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:22.690070   65670 main.go:141] libmachine: Using SSH client type: native
I1121 17:57:22.690262   65670 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1121 17:57:22.690271   65670 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1121 17:57:22.816387   65670 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1121 17:57:22.816425   65670 ubuntu.go:175] set auth options {CertDir:/home/robson/.minikube CaCertPath:/home/robson/.minikube/certs/ca.pem CaPrivateKeyPath:/home/robson/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/robson/.minikube/machines/server.pem ServerKeyPath:/home/robson/.minikube/machines/server-key.pem ClientKeyPath:/home/robson/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/robson/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/robson/.minikube}
I1121 17:57:22.816461   65670 ubuntu.go:177] setting up certificates
I1121 17:57:22.816469   65670 provision.go:84] configureAuth start
I1121 17:57:22.816592   65670 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1121 17:57:54.654476   65670 provision.go:143] copyHostCerts
I1121 17:57:54.654579   65670 exec_runner.go:151] cp: /home/robson/.minikube/certs/ca.pem --> /home/robson/.minikube/ca.pem (1078 bytes)
I1121 17:57:54.654704   65670 exec_runner.go:151] cp: /home/robson/.minikube/certs/cert.pem --> /home/robson/.minikube/cert.pem (1119 bytes)
I1121 17:57:54.654756   65670 exec_runner.go:151] cp: /home/robson/.minikube/certs/key.pem --> /home/robson/.minikube/key.pem (1679 bytes)
I1121 17:57:54.654904   65670 provision.go:117] generating server cert: /home/robson/.minikube/machines/server.pem ca-key=/home/robson/.minikube/certs/ca.pem private-key=/home/robson/.minikube/certs/ca-key.pem org=robson.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1121 17:57:54.805100   65670 provision.go:177] copyRemoteCerts
I1121 17:57:54.805162   65670 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1121 17:57:54.805224   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:54.823120   65670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/robson/.minikube/machines/minikube/id_rsa Username:docker}
I1121 17:57:23.101788   65670 ssh_runner.go:362] scp /home/robson/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1121 17:57:23.127943   65670 ssh_runner.go:362] scp /home/robson/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I1121 17:57:23.156911   65670 ssh_runner.go:362] scp /home/robson/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1121 17:57:23.185010   65670 provision.go:87] duration metric: took 368.545518ms to configureAuth
I1121 17:57:23.185024   65670 ubuntu.go:193] setting minikube options for container-runtime
I1121 17:57:23.185328   65670 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1121 17:57:23.185402   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:23.203195   65670 main.go:141] libmachine: Using SSH client type: native
I1121 17:57:23.203483   65670 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1121 17:57:23.203490   65670 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1121 17:57:23.337472   65670 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1121 17:57:23.337488   65670 ubuntu.go:71] root file system type: overlay
I1121 17:57:23.337770   65670 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1121 17:57:23.337822   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:23.357613   65670 main.go:141] libmachine: Using SSH client type: native
I1121 17:57:23.357748   65670 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1121 17:57:23.357807   65670 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1121 17:57:23.501709   65670 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1121 17:57:23.501898   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:23.520545   65670 main.go:141] libmachine: Using SSH client type: native
I1121 17:57:23.520735   65670 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1121 17:57:23.520746   65670 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1121 17:57:26.372468   65670 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-08-27 14:13:43.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-11-21 20:57:23.490674458 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1121 17:57:26.372486   65670 machine.go:96] duration metric: took 7.037299578s to provisionDockerMachine
I1121 17:57:26.372495   65670 client.go:171] duration metric: took 15.73751997s to LocalClient.Create
I1121 17:57:26.372531   65670 start.go:167] duration metric: took 15.737649026s to libmachine.API.Create "minikube"
I1121 17:57:26.372590   65670 start.go:293] postStartSetup for "minikube" (driver="docker")
I1121 17:57:26.372600   65670 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1121 17:57:26.372663   65670 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1121 17:57:26.372701   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:26.394080   65670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/robson/.minikube/machines/minikube/id_rsa Username:docker}
I1121 17:57:26.493329   65670 ssh_runner.go:195] Run: cat /etc/os-release
I1121 17:57:26.496996   65670 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1121 17:57:26.497012   65670 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1121 17:57:26.497018   65670 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1121 17:57:26.497023   65670 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1121 17:57:26.497059   65670 filesync.go:126] Scanning /home/robson/.minikube/addons for local assets ...
I1121 17:57:26.497694   65670 filesync.go:126] Scanning /home/robson/.minikube/files for local assets ...
I1121 17:57:26.498134   65670 start.go:296] duration metric: took 125.530449ms for postStartSetup
I1121 17:57:26.498627   65670 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1121 17:57:26.518231   65670 profile.go:143] Saving config to /home/robson/.minikube/profiles/minikube/config.json ...
I1121 17:57:26.518715   65670 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1121 17:57:26.518769   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:26.538685   65670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/robson/.minikube/machines/minikube/id_rsa Username:docker}
I1121 17:57:26.627805   65670 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1121 17:57:26.633606   65670 start.go:128] duration metric: took 16.001296571s to createHost
I1121 17:57:26.633620   65670 start.go:83] releasing machines lock for "minikube", held for 16.001951551s
I1121 17:57:26.633680   65670 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1121 17:57:26.652087   65670 ssh_runner.go:195] Run: cat /version.json
I1121 17:57:26.652099   65670 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1121 17:57:26.652124   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:26.652144   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:26.673731   65670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/robson/.minikube/machines/minikube/id_rsa Username:docker}
I1121 17:57:26.675702   65670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/robson/.minikube/machines/minikube/id_rsa Username:docker}
I1121 17:57:27.198277   65670 ssh_runner.go:195] Run: systemctl --version
I1121 17:57:27.203092   65670 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1121 17:57:27.208982   65670 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1121 17:57:27.237197   65670 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1121 17:57:27.237281   65670 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1121 17:57:27.272556   65670 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1121 17:57:27.272636   65670 start.go:495] detecting cgroup driver to use...
I1121 17:57:27.272688   65670 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1121 17:57:27.272970   65670 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1121 17:57:27.293895   65670 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1121 17:57:27.305979   65670 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1121 17:57:27.317846   65670 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1121 17:57:27.317898   65670 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1121 17:57:27.330114   65670 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1121 17:57:27.342421   65670 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1121 17:57:27.354092   65670 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1121 17:57:27.367515   65670 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1121 17:57:27.379880   65670 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1121 17:57:27.391526   65670 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1121 17:57:27.402696   65670 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1121 17:57:27.415981   65670 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1121 17:57:27.425703   65670 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1121 17:57:27.435639   65670 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1121 17:57:27.538924   65670 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1121 17:57:27.701456   65670 start.go:495] detecting cgroup driver to use...
I1121 17:57:27.701577   65670 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1121 17:57:27.701667   65670 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1121 17:57:27.716581   65670 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1121 17:57:27.716689   65670 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1121 17:57:27.732473   65670 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1121 17:57:27.753016   65670 ssh_runner.go:195] Run: which cri-dockerd
I1121 17:57:27.757565   65670 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1121 17:57:27.769905   65670 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1121 17:57:27.791948   65670 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1121 17:57:59.755036   65670 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1121 17:57:28.060284   65670 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1121 17:57:28.060827   65670 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1121 17:57:28.083237   65670 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1121 17:57:28.187724   65670 ssh_runner.go:195] Run: sudo systemctl restart docker
I1121 17:57:32.455012   65670 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.267253211s)
I1121 17:57:32.455094   65670 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1121 17:57:32.468685   65670 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1121 17:57:32.482879   65670 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1121 17:57:32.605368   65670 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1121 17:57:32.716822   65670 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1121 17:57:32.827628   65670 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1121 17:57:32.841948   65670 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1121 17:57:32.855382   65670 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1121 17:58:04.776712   65670 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1121 17:57:33.051617   65670 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1121 17:57:33.051689   65670 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1121 17:57:33.056601   65670 start.go:563] Will wait 60s for crictl version
I1121 17:57:33.056676   65670 ssh_runner.go:195] Run: which crictl
I1121 17:57:33.061048   65670 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1121 17:57:33.108344   65670 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1121 17:57:33.108439   65670 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1121 17:57:33.136553   65670 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1121 17:57:33.168137   65670 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1121 17:57:33.168470   65670 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1121 17:57:33.187161   65670 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1121 17:57:33.191450   65670 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1121 17:57:33.206228   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1121 17:57:33.226750   65670 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/robson:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1121 17:57:33.226944   65670 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1121 17:57:33.227013   65670 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1121 17:57:33.249893   65670 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1121 17:57:33.249960   65670 docker.go:615] Images already preloaded, skipping extraction
I1121 17:57:33.250020   65670 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1121 17:57:33.272785   65670 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1121 17:57:33.272796   65670 cache_images.go:84] Images are preloaded, skipping loading
I1121 17:57:33.272807   65670 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I1121 17:57:33.272900   65670 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1121 17:57:33.272948   65670 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1121 17:57:33.332159   65670 cni.go:84] Creating CNI manager for ""
I1121 17:57:33.332173   65670 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1121 17:57:33.332227   65670 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1121 17:57:33.332244   65670 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1121 17:57:33.332375   65670 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1121 17:57:33.332458   65670 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1121 17:57:33.342954   65670 binaries.go:44] Found k8s binaries, skipping transfer
I1121 17:57:33.343098   65670 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1121 17:57:33.353252   65670 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1121 17:57:33.374023   65670 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1121 17:57:33.392318   65670 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I1121 17:57:33.412872   65670 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1121 17:57:33.417111   65670 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1121 17:57:33.429702   65670 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1121 17:57:33.526366   65670 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1121 17:57:33.541704   65670 certs.go:68] Setting up /home/robson/.minikube/profiles/minikube for IP: 192.168.49.2
I1121 17:57:33.541743   65670 certs.go:194] generating shared ca certs ...
I1121 17:57:33.541769   65670 certs.go:226] acquiring lock for ca certs: {Name:mkb80777938b3e256eb9b54ecfbdf29e0bdedba6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:33.542045   65670 certs.go:240] generating "minikubeCA" ca cert: /home/robson/.minikube/ca.key
I1121 17:57:33.688683   65670 crypto.go:156] Writing cert to /home/robson/.minikube/ca.crt ...
I1121 17:57:33.688699   65670 lock.go:35] WriteFile acquiring /home/robson/.minikube/ca.crt: {Name:mkb43ec644370dc8248b6da29343b81be21ddd6f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:33.688895   65670 crypto.go:164] Writing key to /home/robson/.minikube/ca.key ...
I1121 17:57:33.688902   65670 lock.go:35] WriteFile acquiring /home/robson/.minikube/ca.key: {Name:mk036dd718d26d64b4219284924ef51f0f4df299 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:33.689000   65670 certs.go:240] generating "proxyClientCA" ca cert: /home/robson/.minikube/proxy-client-ca.key
I1121 17:57:33.868686   65670 crypto.go:156] Writing cert to /home/robson/.minikube/proxy-client-ca.crt ...
I1121 17:57:33.868698   65670 lock.go:35] WriteFile acquiring /home/robson/.minikube/proxy-client-ca.crt: {Name:mkadc72b30ccb5421d0d0fd69a8b5860925c2077 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:33.868889   65670 crypto.go:164] Writing key to /home/robson/.minikube/proxy-client-ca.key ...
I1121 17:57:33.868894   65670 lock.go:35] WriteFile acquiring /home/robson/.minikube/proxy-client-ca.key: {Name:mkee579d0eb562370a55b81769103cd4db0dd0b1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:33.868986   65670 certs.go:256] generating profile certs ...
I1121 17:57:33.869079   65670 certs.go:363] generating signed profile cert for "minikube-user": /home/robson/.minikube/profiles/minikube/client.key
I1121 17:57:33.869089   65670 crypto.go:68] Generating cert /home/robson/.minikube/profiles/minikube/client.crt with IP's: []
I1121 17:57:34.001523   65670 crypto.go:156] Writing cert to /home/robson/.minikube/profiles/minikube/client.crt ...
I1121 17:57:34.001537   65670 lock.go:35] WriteFile acquiring /home/robson/.minikube/profiles/minikube/client.crt: {Name:mk0c9b912e751bbe3e245882c252752975872cc3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:34.001753   65670 crypto.go:164] Writing key to /home/robson/.minikube/profiles/minikube/client.key ...
I1121 17:57:34.001760   65670 lock.go:35] WriteFile acquiring /home/robson/.minikube/profiles/minikube/client.key: {Name:mk2bb55cb05c56d19d399014081f1ef8938d9661 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:34.001856   65670 certs.go:363] generating signed profile cert for "minikube": /home/robson/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1121 17:57:34.001869   65670 crypto.go:68] Generating cert /home/robson/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I1121 17:57:34.051828   65670 crypto.go:156] Writing cert to /home/robson/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I1121 17:57:34.051842   65670 lock.go:35] WriteFile acquiring /home/robson/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk3886d83f63b256a10092e2062f315612e32ec7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:34.052034   65670 crypto.go:164] Writing key to /home/robson/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I1121 17:57:34.052039   65670 lock.go:35] WriteFile acquiring /home/robson/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk76cfe834294aff0926e0d5c6e2b09e529667d7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:34.052159   65670 certs.go:381] copying /home/robson/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/robson/.minikube/profiles/minikube/apiserver.crt
I1121 17:57:34.052227   65670 certs.go:385] copying /home/robson/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/robson/.minikube/profiles/minikube/apiserver.key
I1121 17:57:34.052278   65670 certs.go:363] generating signed profile cert for "aggregator": /home/robson/.minikube/profiles/minikube/proxy-client.key
I1121 17:57:34.052290   65670 crypto.go:68] Generating cert /home/robson/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1121 17:57:34.352850   65670 crypto.go:156] Writing cert to /home/robson/.minikube/profiles/minikube/proxy-client.crt ...
I1121 17:57:34.352863   65670 lock.go:35] WriteFile acquiring /home/robson/.minikube/profiles/minikube/proxy-client.crt: {Name:mkab52fefcd9b03367f237305b427add260de0a1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:34.353059   65670 crypto.go:164] Writing key to /home/robson/.minikube/profiles/minikube/proxy-client.key ...
I1121 17:57:34.353065   65670 lock.go:35] WriteFile acquiring /home/robson/.minikube/profiles/minikube/proxy-client.key: {Name:mk10c5e053e37d06d68ec5cd91a4b070ee57aef7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:34.353249   65670 certs.go:484] found cert: /home/robson/.minikube/certs/ca-key.pem (1679 bytes)
I1121 17:57:34.353273   65670 certs.go:484] found cert: /home/robson/.minikube/certs/ca.pem (1078 bytes)
I1121 17:57:34.353288   65670 certs.go:484] found cert: /home/robson/.minikube/certs/cert.pem (1119 bytes)
I1121 17:57:34.353302   65670 certs.go:484] found cert: /home/robson/.minikube/certs/key.pem (1679 bytes)
I1121 17:57:34.356011   65670 ssh_runner.go:362] scp /home/robson/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1121 17:57:34.383376   65670 ssh_runner.go:362] scp /home/robson/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1121 17:57:34.409084   65670 ssh_runner.go:362] scp /home/robson/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1121 17:57:34.435854   65670 ssh_runner.go:362] scp /home/robson/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1121 17:57:34.463128   65670 ssh_runner.go:362] scp /home/robson/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1121 17:57:34.490041   65670 ssh_runner.go:362] scp /home/robson/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1121 17:57:34.515931   65670 ssh_runner.go:362] scp /home/robson/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1121 17:57:34.544854   65670 ssh_runner.go:362] scp /home/robson/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1121 17:57:34.574156   65670 ssh_runner.go:362] scp /home/robson/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1121 17:57:34.601045   65670 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1121 17:57:34.622599   65670 ssh_runner.go:195] Run: openssl version
I1121 17:57:34.628472   65670 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1121 17:57:34.641335   65670 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1121 17:57:34.646145   65670 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 21 20:57 /usr/share/ca-certificates/minikubeCA.pem
I1121 17:57:34.646194   65670 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1121 17:57:34.653232   65670 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1121 17:57:34.663324   65670 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1121 17:57:34.667791   65670 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1121 17:57:34.667824   65670 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/robson:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1121 17:57:34.667920   65670 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1121 17:57:34.690606   65670 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1121 17:57:34.700415   65670 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1121 17:57:34.710330   65670 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I1121 17:57:34.710380   65670 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1121 17:57:34.719732   65670 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1121 17:57:34.719739   65670 kubeadm.go:157] found existing configuration files:

I1121 17:57:34.719783   65670 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1121 17:57:34.730219   65670 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1121 17:57:34.730274   65670 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1121 17:57:34.740199   65670 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1121 17:57:34.749141   65670 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1121 17:57:34.749206   65670 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1121 17:57:34.759437   65670 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1121 17:57:34.768482   65670 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1121 17:57:34.768652   65670 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1121 17:57:34.780407   65670 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1121 17:57:34.790452   65670 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1121 17:57:34.790499   65670 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1121 17:57:34.800352   65670 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1121 17:57:34.842217   65670 kubeadm.go:310] W1121 20:57:34.841341    1947 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1121 17:57:34.842496   65670 kubeadm.go:310] W1121 20:57:34.841899    1947 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I1121 17:57:34.862739   65670 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I1121 17:57:34.934325   65670 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1121 17:57:44.351871   65670 kubeadm.go:310] [init] Using Kubernetes version: v1.31.0
I1121 17:57:44.351907   65670 kubeadm.go:310] [preflight] Running pre-flight checks
I1121 17:57:44.351959   65670 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1121 17:57:44.352055   65670 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1121 17:57:44.352143   65670 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1121 17:57:44.352234   65670 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1121 17:57:44.354024   65670 out.go:235]     ▪ Generating certificates and keys ...
I1121 17:57:44.354225   65670 kubeadm.go:310] [certs] Using existing ca certificate authority
I1121 17:57:44.354272   65670 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1121 17:57:44.354318   65670 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1121 17:57:44.354399   65670 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1121 17:57:44.354449   65670 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1121 17:57:44.354503   65670 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1121 17:57:44.354551   65670 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1121 17:57:44.354692   65670 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1121 17:57:44.354738   65670 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1121 17:57:44.354814   65670 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1121 17:57:44.354888   65670 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1121 17:57:44.354959   65670 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1121 17:57:44.355015   65670 kubeadm.go:310] [certs] Generating "sa" key and public key
I1121 17:57:44.355055   65670 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1121 17:57:44.355107   65670 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1121 17:57:44.355145   65670 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1121 17:57:44.355216   65670 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1121 17:57:44.355346   65670 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1121 17:57:44.355395   65670 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1121 17:57:44.355494   65670 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1121 17:57:44.355553   65670 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1121 17:57:44.358404   65670 out.go:235]     ▪ Booting up control plane ...
I1121 17:57:44.358576   65670 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1121 17:57:44.358640   65670 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1121 17:57:44.358701   65670 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1121 17:57:44.358822   65670 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1121 17:57:44.358908   65670 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1121 17:57:44.358937   65670 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1121 17:57:44.359065   65670 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1121 17:57:44.359156   65670 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1121 17:57:44.359196   65670 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.95936ms
I1121 17:57:44.359269   65670 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I1121 17:57:44.359308   65670 kubeadm.go:310] [api-check] The API server is healthy after 5.50150854s
I1121 17:57:44.359379   65670 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1121 17:57:44.359482   65670 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1121 17:57:44.359535   65670 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1121 17:57:44.359655   65670 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1121 17:57:44.359715   65670 kubeadm.go:310] [bootstrap-token] Using token: n8yis9.7kvmcbxv1x4mrx8d
I1121 17:57:44.361523   65670 out.go:235]     ▪ Configuring RBAC rules ...
I1121 17:57:44.361710   65670 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1121 17:57:44.361769   65670 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1121 17:57:44.361863   65670 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1121 17:57:44.361999   65670 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1121 17:57:44.362120   65670 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1121 17:57:44.362204   65670 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1121 17:57:44.362322   65670 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1121 17:57:44.362358   65670 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1121 17:57:44.362399   65670 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1121 17:57:44.362405   65670 kubeadm.go:310] 
I1121 17:57:44.362454   65670 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1121 17:57:44.362459   65670 kubeadm.go:310] 
I1121 17:57:44.362545   65670 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1121 17:57:44.362549   65670 kubeadm.go:310] 
I1121 17:57:44.362566   65670 kubeadm.go:310]   mkdir -p $HOME/.kube
I1121 17:57:44.362605   65670 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1121 17:57:44.362667   65670 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1121 17:57:44.362670   65670 kubeadm.go:310] 
I1121 17:57:44.362706   65670 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1121 17:57:44.362731   65670 kubeadm.go:310] 
I1121 17:57:44.362764   65670 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1121 17:57:44.362766   65670 kubeadm.go:310] 
I1121 17:57:44.362801   65670 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1121 17:57:44.362851   65670 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1121 17:57:44.362919   65670 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1121 17:57:44.362922   65670 kubeadm.go:310] 
I1121 17:57:44.362979   65670 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1121 17:57:44.363069   65670 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1121 17:57:44.363075   65670 kubeadm.go:310] 
I1121 17:57:44.363159   65670 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token n8yis9.7kvmcbxv1x4mrx8d \
I1121 17:57:44.363283   65670 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:3d88d0443c90da1cb9e4d27ed873e2b76471b87ee34b87a10226b3d29794da01 \
I1121 17:57:44.363299   65670 kubeadm.go:310] 	--control-plane 
I1121 17:57:44.363302   65670 kubeadm.go:310] 
I1121 17:57:44.363359   65670 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1121 17:57:44.363361   65670 kubeadm.go:310] 
I1121 17:57:44.363439   65670 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token n8yis9.7kvmcbxv1x4mrx8d \
I1121 17:57:44.363527   65670 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:3d88d0443c90da1cb9e4d27ed873e2b76471b87ee34b87a10226b3d29794da01 
I1121 17:57:44.363533   65670 cni.go:84] Creating CNI manager for ""
I1121 17:57:44.363544   65670 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1121 17:57:44.367558   65670 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I1121 17:57:44.370550   65670 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1121 17:57:44.380435   65670 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1121 17:57:44.442411   65670 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1121 17:57:44.442548   65670 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1121 17:57:44.442650   65670 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_11_21T17_57_44_0700 minikube.k8s.io/version=v1.34.0 minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1121 17:57:44.452307   65670 ops.go:34] apiserver oom_adj: -16
I1121 17:57:44.556935   65670 kubeadm.go:1113] duration metric: took 114.440821ms to wait for elevateKubeSystemPrivileges
I1121 17:57:44.580084   65670 kubeadm.go:394] duration metric: took 9.911284772s to StartCluster
I1121 17:57:44.580116   65670 settings.go:142] acquiring lock: {Name:mkf4a6ebe50f8c1dc6b61c0774a841b2bf395c5a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:44.580290   65670 settings.go:150] Updating kubeconfig:  /home/robson/.kube/config
I1121 17:57:44.581396   65670 lock.go:35] WriteFile acquiring /home/robson/.kube/config: {Name:mkdd149d965989e9e2af9ecc9a9c8d25f881b378 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1121 17:57:44.581746   65670 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1121 17:57:44.581774   65670 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1121 17:57:44.581977   65670 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1121 17:57:44.582168   65670 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1121 17:57:44.582386   65670 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I1121 17:57:44.582466   65670 host.go:66] Checking if "minikube" exists ...
I1121 17:57:44.582626   65670 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1121 17:57:44.582679   65670 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1121 17:57:44.582786   65670 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1121 17:57:44.583123   65670 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1121 17:57:44.583258   65670 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1121 17:57:44.585196   65670 out.go:177] 🔎  Verifying Kubernetes components...
I1121 17:57:44.588809   65670 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1121 17:57:44.610968   65670 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1121 17:57:44.613584   65670 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1121 17:57:44.613624   65670 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1121 17:57:44.613686   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:44.625088   65670 addons.go:234] Setting addon default-storageclass=true in "minikube"
I1121 17:57:44.625135   65670 host.go:66] Checking if "minikube" exists ...
I1121 17:57:44.625490   65670 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1121 17:57:44.637735   65670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/robson/.minikube/machines/minikube/id_rsa Username:docker}
I1121 17:57:44.652802   65670 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1121 17:57:44.652860   65670 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1121 17:57:44.652932   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1121 17:57:44.678939   65670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/robson/.minikube/machines/minikube/id_rsa Username:docker}
I1121 17:57:44.762600   65670 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1121 17:57:44.843119   65670 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1121 17:57:44.855350   65670 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1121 17:57:44.855350   65670 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1121 17:57:45.337818   65670 start.go:971] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I1121 17:57:45.337967   65670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1121 17:57:45.362063   65670 api_server.go:52] waiting for apiserver process to appear ...
I1121 17:57:45.362154   65670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1121 17:57:45.534275   65670 api_server.go:72] duration metric: took 952.461816ms to wait for apiserver process to appear ...
I1121 17:57:45.534294   65670 api_server.go:88] waiting for apiserver healthz status ...
I1121 17:57:45.534320   65670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32771/healthz ...
I1121 17:57:45.539359   65670 api_server.go:279] https://127.0.0.1:32771/healthz returned 200:
ok
I1121 17:57:45.541356   65670 api_server.go:141] control plane version: v1.31.0
I1121 17:57:45.541384   65670 api_server.go:131] duration metric: took 7.082473ms to wait for apiserver health ...
I1121 17:57:45.541406   65670 system_pods.go:43] waiting for kube-system pods to appear ...
I1121 17:57:45.548970   65670 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I1121 17:57:45.549475   65670 system_pods.go:59] 5 kube-system pods found
I1121 17:57:45.549502   65670 system_pods.go:61] "etcd-minikube" [0a0725d3-4e2d-4322-806b-ad30e0a7f278] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1121 17:57:45.549506   65670 system_pods.go:61] "kube-apiserver-minikube" [a649fa41-7d83-44a4-a8f7-5a063f67c700] Running
I1121 17:57:45.549515   65670 system_pods.go:61] "kube-controller-manager-minikube" [882a3adc-686e-4ca1-8540-c27897691e3d] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1121 17:57:45.549522   65670 system_pods.go:61] "kube-scheduler-minikube" [5594c5a8-0306-4293-89d5-8d13cf280ea8] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1121 17:57:45.549531   65670 system_pods.go:61] "storage-provisioner" [e7b029f0-83e5-4e8a-b314-fc4cfda721eb] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I1121 17:57:45.549540   65670 system_pods.go:74] duration metric: took 8.127587ms to wait for pod list to return data ...
I1121 17:57:45.549553   65670 kubeadm.go:582] duration metric: took 967.748502ms to wait for: map[apiserver:true system_pods:true]
I1121 17:57:45.549600   65670 node_conditions.go:102] verifying NodePressure condition ...
I1121 17:57:45.551245   65670 addons.go:510] duration metric: took 969.357873ms for enable addons: enabled=[storage-provisioner default-storageclass]
I1121 17:57:45.555457   65670 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1121 17:57:45.555475   65670 node_conditions.go:123] node cpu capacity is 12
I1121 17:57:45.555494   65670 node_conditions.go:105] duration metric: took 5.89102ms to run NodePressure ...
I1121 17:57:45.555505   65670 start.go:241] waiting for startup goroutines ...
I1121 17:57:45.842972   65670 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1121 17:57:45.843008   65670 start.go:246] waiting for cluster config update ...
I1121 17:57:45.843017   65670 start.go:255] writing updated cluster config ...
I1121 17:57:45.843291   65670 ssh_runner.go:195] Run: rm -f paused
I1121 17:57:46.076484   65670 start.go:600] kubectl: 1.31.1, cluster: 1.31.0 (minor skew: 0)
I1121 17:57:46.078688   65670 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Nov 21 20:59:27 minikube cri-dockerd[1630]: time="2024-11-21T20:59:27Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 20:59:27 minikube cri-dockerd[1630]: time="2024-11-21T20:59:27Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 20:59:39 minikube cri-dockerd[1630]: time="2024-11-21T20:59:39Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 20:59:39 minikube cri-dockerd[1630]: time="2024-11-21T20:59:39Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:00:14 minikube cri-dockerd[1630]: time="2024-11-21T21:00:14Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:00:14 minikube cri-dockerd[1630]: time="2024-11-21T21:00:14Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:01:33 minikube cri-dockerd[1630]: time="2024-11-21T21:01:33Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:01:33 minikube cri-dockerd[1630]: time="2024-11-21T21:01:33Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:03:57 minikube cri-dockerd[1630]: time="2024-11-21T21:03:57Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:03:57 minikube cri-dockerd[1630]: time="2024-11-21T21:03:57Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:04:11 minikube cri-dockerd[1630]: time="2024-11-21T21:04:11Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:04:11 minikube cri-dockerd[1630]: time="2024-11-21T21:04:11Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:04:23 minikube cri-dockerd[1630]: time="2024-11-21T21:04:23Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:04:23 minikube cri-dockerd[1630]: time="2024-11-21T21:04:23Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:04:34 minikube cri-dockerd[1630]: time="2024-11-21T21:04:34Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:04:34 minikube cri-dockerd[1630]: time="2024-11-21T21:04:34Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:04:46 minikube cri-dockerd[1630]: time="2024-11-21T21:04:46Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:04:46 minikube cri-dockerd[1630]: time="2024-11-21T21:04:46Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:04:57 minikube cri-dockerd[1630]: time="2024-11-21T21:04:57Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:04:57 minikube cri-dockerd[1630]: time="2024-11-21T21:04:57Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:05:11 minikube cri-dockerd[1630]: time="2024-11-21T21:05:11Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:05:11 minikube cri-dockerd[1630]: time="2024-11-21T21:05:11Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:05:26 minikube cri-dockerd[1630]: time="2024-11-21T21:05:26Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:05:26 minikube cri-dockerd[1630]: time="2024-11-21T21:05:26Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:06:10 minikube cri-dockerd[1630]: time="2024-11-21T21:06:10Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:06:10 minikube cri-dockerd[1630]: time="2024-11-21T21:06:10Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:05:51 minikube cri-dockerd[1630]: time="2024-11-21T21:05:51Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:05:51 minikube cri-dockerd[1630]: time="2024-11-21T21:05:51Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:05:59 minikube cri-dockerd[1630]: time="2024-11-21T21:05:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f4b3e7207f18a01c725b3d3abe25e1a2dcc3b3e3c080a002932602b199b93986/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local btfinanceira.com.br options ndots:5]"
Nov 21 21:05:59 minikube cri-dockerd[1630]: time="2024-11-21T21:05:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/60f2ddbb6c60d19a5b663d7792efdfd8d5f126d4155b7bbfd0e89575b1e7e880/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local btfinanceira.com.br options ndots:5]"
Nov 21 21:06:05 minikube cri-dockerd[1630]: time="2024-11-21T21:06:05Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:06:05 minikube cri-dockerd[1630]: time="2024-11-21T21:06:05Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:06:09 minikube cri-dockerd[1630]: time="2024-11-21T21:06:09Z" level=info msg="Stop pulling image rpko777/hello-world-robson:latest: Status: Downloaded newer image for rpko777/hello-world-robson:latest"
Nov 21 21:06:10 minikube cri-dockerd[1630]: time="2024-11-21T21:06:10Z" level=info msg="Stop pulling image rpko777/hello-world-robson:latest: Status: Image is up to date for rpko777/hello-world-robson:latest"
Nov 21 21:06:19 minikube cri-dockerd[1630]: time="2024-11-21T21:06:19Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:06:19 minikube cri-dockerd[1630]: time="2024-11-21T21:06:19Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:06:30 minikube cri-dockerd[1630]: time="2024-11-21T21:06:30Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:06:30 minikube cri-dockerd[1630]: time="2024-11-21T21:06:30Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:06:42 minikube cri-dockerd[1630]: time="2024-11-21T21:06:42Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:06:42 minikube cri-dockerd[1630]: time="2024-11-21T21:06:42Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:06:54 minikube cri-dockerd[1630]: time="2024-11-21T21:06:54Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:06:54 minikube cri-dockerd[1630]: time="2024-11-21T21:06:54Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:07:40 minikube cri-dockerd[1630]: time="2024-11-21T21:07:40Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:07:40 minikube cri-dockerd[1630]: time="2024-11-21T21:07:40Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:07:55 minikube cri-dockerd[1630]: time="2024-11-21T21:07:55Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:07:55 minikube cri-dockerd[1630]: time="2024-11-21T21:07:55Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:07:36 minikube cri-dockerd[1630]: time="2024-11-21T21:07:36Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:07:36 minikube cri-dockerd[1630]: time="2024-11-21T21:07:36Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:07:51 minikube cri-dockerd[1630]: time="2024-11-21T21:07:51Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:07:51 minikube cri-dockerd[1630]: time="2024-11-21T21:07:51Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:08:04 minikube cri-dockerd[1630]: time="2024-11-21T21:08:04Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:08:04 minikube cri-dockerd[1630]: time="2024-11-21T21:08:04Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:08:19 minikube cri-dockerd[1630]: time="2024-11-21T21:08:19Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:08:19 minikube cri-dockerd[1630]: time="2024-11-21T21:08:19Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:08:30 minikube cri-dockerd[1630]: time="2024-11-21T21:08:30Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:08:30 minikube cri-dockerd[1630]: time="2024-11-21T21:08:30Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:08:41 minikube cri-dockerd[1630]: time="2024-11-21T21:08:41Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:08:41 minikube cri-dockerd[1630]: time="2024-11-21T21:08:41Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"
Nov 21 21:08:55 minikube cri-dockerd[1630]: time="2024-11-21T21:08:55Z" level=info msg="Unable to create pod sandbox due to conflict. Attempting to remove sandbox. Container c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b"
Nov 21 21:08:55 minikube cri-dockerd[1630]: time="2024-11-21T21:08:55Z" level=error msg="Failed to remove the conflicting container (c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b): Error response from daemon: cannot remove container \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\": container is running: stop the container before removing or force remove"


==> container status <==
CONTAINER           IMAGE                                                                                                CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
8be3e7f3d3652       rpko777/hello-world-robson@sha256:5495847d5602c409668874985fe0aa5d72154bbf81c94c940ec14df448adabe3   2 minutes ago       Running             hello-world-robson        0                   60f2ddbb6c60d       hello-world-robson-677967b8b7-wn9x4
a6a8dcd873b81       rpko777/hello-world-robson@sha256:5495847d5602c409668874985fe0aa5d72154bbf81c94c940ec14df448adabe3   2 minutes ago       Running             hello-world-robson        0                   f4b3e7207f18a       hello-world-robson-677967b8b7-wn2b6
64cb52c29a0f3       6e38f40d628db                                                                                        10 minutes ago      Exited              storage-provisioner       0                   670adcc699129       storage-provisioner
c85884823201a       6e38f40d628db                                                                                        10 minutes ago      Running             storage-provisioner       1                   670adcc699129       storage-provisioner
775bfdcf21f13       cbb01a7bd410d                                                                                        11 minutes ago      Running             coredns                   0                   1b969f1f55880       coredns-6f6b679f8f-g4hj2
91caf91f77acd       ad83b2ca7b09e                                                                                        11 minutes ago      Running             kube-proxy                0                   0804dd97fa4a5       kube-proxy-hd7qm
eea90b98ece93       045733566833c                                                                                        11 minutes ago      Running             kube-controller-manager   0                   7947e8e4de3a8       kube-controller-manager-minikube
0847ca281ec8e       1766f54c897f0                                                                                        11 minutes ago      Running             kube-scheduler            0                   cd9baca7218aa       kube-scheduler-minikube
3656ee2bbb39b       604f5db92eaa8                                                                                        11 minutes ago      Running             kube-apiserver            0                   93f75c3876cc5       kube-apiserver-minikube
18418a2cbccb2       2e96e5913fc06                                                                                        11 minutes ago      Running             etcd                      0                   b1e5f7d4d4f0d       etcd-minikube


==> coredns [775bfdcf21f1] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:57696 - 14480 "HINFO IN 1039295234258662223.8920921396023118958. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.165685446s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[784829]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (21-Nov-2024 20:57:49.842) (total time: 30000ms):
Trace[784829]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (20:58:19.709)
Trace[784829]: [30.000988648s] [30.000988648s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[21370545]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (21-Nov-2024 20:57:49.842) (total time: 30000ms):
Trace[21370545]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (20:58:19.709)
Trace[21370545]: [30.000981094s] [30.000981094s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1548843766]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (21-Nov-2024 20:57:49.842) (total time: 30001ms):
Trace[1548843766]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (20:58:19.709)
Trace[1548843766]: [30.00118906s] [30.00118906s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_11_21T17_57_44_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 21 Nov 2024 20:57:40 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 21 Nov 2024 21:08:57 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 21 Nov 2024 21:06:14 +0000   Thu, 21 Nov 2024 20:57:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 21 Nov 2024 21:06:14 +0000   Thu, 21 Nov 2024 20:57:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 21 Nov 2024 21:06:14 +0000   Thu, 21 Nov 2024 20:57:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 21 Nov 2024 21:06:14 +0000   Thu, 21 Nov 2024 20:57:40 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16055472Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16055472Ki
  pods:               110
System Info:
  Machine ID:                 98807fc4098f49b999ec4494fc012f3e
  System UUID:                98807fc4098f49b999ec4494fc012f3e
  Boot ID:                    fc617ea3-8237-49a6-9d7a-26249c52477e
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---
  default                     hello-world-robson-677967b8b7-wn2b6    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m8s
  default                     hello-world-robson-677967b8b7-wn9x4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m8s
  kube-system                 coredns-6f6b679f8f-g4hj2               100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     11m
  kube-system                 etcd-minikube                          100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         11m
  kube-system                 kube-apiserver-minikube                250m (2%)     0 (0%)      0 (0%)           0 (0%)         11m
  kube-system                 kube-controller-manager-minikube       200m (1%)     0 (0%)      0 (0%)           0 (0%)         11m
  kube-system                 kube-proxy-hd7qm                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  kube-system                 kube-scheduler-minikube                100m (0%)     0 (0%)      0 (0%)           0 (0%)         11m
  kube-system                 storage-provisioner                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           11m                kube-proxy       
  Warning  CgroupV1                           11m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory            11m (x7 over 11m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              11m (x7 over 11m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               11m (x7 over 11m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            11m                kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  11m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           11m                kubelet          Starting kubelet.
  Warning  CgroupV1                           11m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced            11m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            11m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              11m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               11m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     11m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Nov21 16:58]   #2  #3  #4  #5  #6  #7  #8  #9 #10 #11
[  +0.031638] PCI: Fatal: No config space access function found
[  +0.031135] PCI: System does not support PCI
[  +0.035413] kvm: no hardware support
[  +0.814710] FS-Cache: Duplicate cookie detected
[  +0.000469] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000554] FS-Cache: O-cookie d=0000000068c354dd{9P.session} n=00000000bd3721c3
[  +0.000721] FS-Cache: O-key=[10] '34323934393337333838'
[  +0.000432] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000631] FS-Cache: N-cookie d=0000000068c354dd{9P.session} n=00000000b646177c
[  +0.000597] FS-Cache: N-key=[10] '34323934393337333838'
[  +0.684244] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.011573] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000796] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000639] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000506] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000584] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002607] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000702] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000833] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000699] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000578] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.968351] overlayfs: missing 'lowerdir'
[Nov21 20:57] tmpfs: Unknown parameter 'noswap'
[  +6.468776] tmpfs: Unknown parameter 'noswap'


==> etcd [18418a2cbccb] <==
{"level":"warn","ts":"2024-11-21T20:57:38.453617Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-21T20:57:38.453816Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-11-21T20:57:38.453962Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-21T20:57:38.454008Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-11-21T20:57:38.454075Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-21T20:57:38.455144Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-11-21T20:57:38.455360Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-11-21T20:57:38.531712Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"74.716278ms"}
{"level":"info","ts":"2024-11-21T20:57:38.539387Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2024-11-21T20:57:38.539510Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-11-21T20:57:38.539542Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2024-11-21T20:57:38.539550Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-11-21T20:57:38.539556Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2024-11-21T20:57:38.539584Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2024-11-21T20:57:38.546661Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-11-21T20:57:38.550683Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-11-21T20:57:38.553349Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-11-21T20:57:38.556296Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-11-21T20:57:38.556684Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-11-21T20:57:38.556969Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-21T20:57:38.557202Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-21T20:57:38.557221Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-21T20:57:38.557030Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-21T20:57:38.558431Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-11-21T20:57:38.558628Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-11-21T20:57:38.559300Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-21T20:57:38.559537Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-21T20:57:38.559589Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-21T20:57:38.559607Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-11-21T20:57:38.559642Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-11-21T20:57:39.140110Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-11-21T20:57:39.140194Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-11-21T20:57:39.140348Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-11-21T20:57:39.140413Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-11-21T20:57:39.140425Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-11-21T20:57:39.140436Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-11-21T20:57:39.140448Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-11-21T20:57:39.141853Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-11-21T20:57:39.141895Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-21T20:57:39.141946Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-21T20:57:39.141920Z","caller":"etcdserver/server.go:2629","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-21T20:57:39.142482Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-11-21T20:57:39.142576Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-11-21T20:57:39.143100Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-21T20:57:39.143129Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-21T20:57:39.143258Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-21T20:57:39.143338Z","caller":"etcdserver/server.go:2653","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-21T20:57:39.143850Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-21T20:57:39.143923Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-11-21T20:57:39.144766Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-11-21T21:03:13.849762Z","caller":"traceutil/trace.go:171","msg":"trace[1052550996] transaction","detail":"{read_only:false; response_revision:695; number_of_response:1; }","duration":"174.878385ms","start":"2024-11-21T21:03:13.674865Z","end":"2024-11-21T21:03:13.849743Z","steps":["trace[1052550996] 'process raft request'  (duration: 174.764648ms)"],"step_count":1}
{"level":"info","ts":"2024-11-21T21:07:39.013541Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":664}
{"level":"info","ts":"2024-11-21T21:07:39.018994Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":664,"took":"5.116412ms","hash":1669484741,"current-db-size-bytes":1667072,"current-db-size":"1.7 MB","current-db-size-in-use-bytes":1667072,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-11-21T21:07:39.019051Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1669484741,"revision":664,"compact-revision":-1}


==> kernel <==
 21:09:06 up  4:10,  0 users,  load average: 0.29, 0.22, 0.25
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [3656ee2bbb39] <==
I1121 20:57:40.751949       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1121 20:57:40.752078       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1121 20:57:40.752151       1 controller.go:78] Starting OpenAPI AggregationController
I1121 20:57:40.752169       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1121 20:57:40.752194       1 local_available_controller.go:156] Starting LocalAvailability controller
I1121 20:57:40.752218       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1121 20:57:40.752276       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1121 20:57:40.752296       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1121 20:57:40.752325       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1121 20:57:40.752337       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1121 20:57:40.752343       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1121 20:57:40.752120       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1121 20:57:40.752130       1 controller.go:119] Starting legacy_token_tracking_controller
I1121 20:57:40.752556       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1121 20:57:40.752873       1 aggregator.go:169] waiting for initial CRD sync...
I1121 20:57:40.753036       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1121 20:57:40.755504       1 controller.go:142] Starting OpenAPI controller
I1121 20:57:40.755568       1 controller.go:90] Starting OpenAPI V3 controller
I1121 20:57:40.755585       1 naming_controller.go:294] Starting NamingConditionController
I1121 20:57:40.755604       1 establishing_controller.go:81] Starting EstablishingController
I1121 20:57:40.755646       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1121 20:57:40.755665       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1121 20:57:40.755677       1 crd_finalizer.go:269] Starting CRDFinalizer
I1121 20:57:40.767242       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1121 20:57:40.767308       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1121 20:57:40.927616       1 cache.go:39] Caches are synced for LocalAvailability controller
I1121 20:57:40.927689       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1121 20:57:40.927710       1 policy_source.go:224] refreshing policies
I1121 20:57:40.927924       1 shared_informer.go:320] Caches are synced for node_authorizer
I1121 20:57:40.927990       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1121 20:57:40.928017       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1121 20:57:40.928027       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1121 20:57:40.928069       1 aggregator.go:171] initial CRD sync complete...
I1121 20:57:40.928080       1 autoregister_controller.go:144] Starting autoregister controller
I1121 20:57:40.928095       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1121 20:57:40.928102       1 cache.go:39] Caches are synced for autoregister controller
I1121 20:57:40.928148       1 shared_informer.go:320] Caches are synced for configmaps
I1121 20:57:40.931395       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1121 20:57:40.931425       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1121 20:57:40.931506       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1121 20:57:40.931587       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1121 20:57:40.933173       1 controller.go:615] quota admission added evaluator for: namespaces
E1121 20:57:40.934219       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I1121 20:57:41.141724       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1121 20:57:41.770901       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1121 20:57:41.779421       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1121 20:57:41.779479       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1121 20:57:42.447295       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1121 20:57:42.515915       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1121 20:57:42.640832       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1121 20:57:42.649384       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1121 20:57:42.650481       1 controller.go:615] quota admission added evaluator for: endpoints
I1121 20:57:42.656381       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1121 20:57:42.836433       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I1121 20:57:43.742687       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1121 20:57:43.755499       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1121 20:57:43.765220       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I1121 20:57:48.489356       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I1121 20:57:48.540293       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1121 21:06:06.054594       1 alloc.go:330] "allocated clusterIPs" service="default/hello-world-robson-service" clusterIPs={"IPv4":"10.99.179.71"}


==> kube-controller-manager [eea90b98ece9] <==
I1121 20:57:47.539810       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I1121 20:57:47.539832       1 shared_informer.go:320] Caches are synced for service account
I1121 20:57:47.540200       1 shared_informer.go:320] Caches are synced for ReplicationController
I1121 20:57:47.541299       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1121 20:57:47.541453       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1121 20:57:47.543900       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I1121 20:57:47.544150       1 shared_informer.go:320] Caches are synced for job
I1121 20:57:47.547920       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I1121 20:57:47.548051       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I1121 20:57:47.548563       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1121 20:57:47.548881       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1121 20:57:47.550177       1 shared_informer.go:320] Caches are synced for PV protection
I1121 20:57:47.552527       1 shared_informer.go:320] Caches are synced for TTL after finished
I1121 20:57:47.561646       1 shared_informer.go:320] Caches are synced for ephemeral
I1121 20:57:47.571489       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1121 20:57:47.581994       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1121 20:57:47.586818       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1121 20:57:47.587695       1 shared_informer.go:320] Caches are synced for disruption
I1121 20:57:47.589063       1 shared_informer.go:320] Caches are synced for daemon sets
I1121 20:57:47.589123       1 shared_informer.go:320] Caches are synced for HPA
I1121 20:57:47.589190       1 shared_informer.go:320] Caches are synced for expand
I1121 20:57:47.589203       1 shared_informer.go:320] Caches are synced for stateful set
I1121 20:57:47.589253       1 shared_informer.go:320] Caches are synced for GC
I1121 20:57:47.590443       1 shared_informer.go:320] Caches are synced for deployment
I1121 20:57:47.590899       1 shared_informer.go:320] Caches are synced for TTL
I1121 20:57:47.594734       1 shared_informer.go:320] Caches are synced for namespace
I1121 20:57:47.595972       1 shared_informer.go:320] Caches are synced for node
I1121 20:57:47.596051       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I1121 20:57:47.596109       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I1121 20:57:47.596121       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I1121 20:57:47.596129       1 shared_informer.go:320] Caches are synced for cidrallocator
I1121 20:57:47.603192       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1121 20:57:47.603275       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1121 20:57:47.603337       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1121 20:57:47.640095       1 shared_informer.go:320] Caches are synced for attach detach
I1121 20:57:47.750188       1 shared_informer.go:320] Caches are synced for resource quota
I1121 20:57:47.791066       1 shared_informer.go:320] Caches are synced for resource quota
I1121 20:57:48.195660       1 shared_informer.go:320] Caches are synced for garbage collector
I1121 20:57:48.195730       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1121 20:57:48.203972       1 shared_informer.go:320] Caches are synced for garbage collector
I1121 20:57:48.596066       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1121 20:57:48.699475       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="154.809003ms"
I1121 20:57:48.708238       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="8.707243ms"
I1121 20:57:48.708365       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="53.402µs"
I1121 20:57:48.714772       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="50.636µs"
I1121 20:57:50.228974       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="80.474µs"
I1121 20:57:54.227273       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1121 20:58:30.055489       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="11.072686ms"
I1121 20:58:30.055683       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="115.891µs"
I1121 21:03:00.221152       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1121 21:05:58.948017       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-world-robson-677967b8b7" duration="31.260436ms"
I1121 21:05:58.985636       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-world-robson-677967b8b7" duration="37.528675ms"
I1121 21:05:58.985761       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-world-robson-677967b8b7" duration="73.831µs"
I1121 21:05:58.986224       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-world-robson-677967b8b7" duration="53.883µs"
I1121 21:05:59.004200       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-world-robson-677967b8b7" duration="63.542µs"
I1121 21:06:09.986945       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-world-robson-677967b8b7" duration="8.85237ms"
I1121 21:06:09.987084       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-world-robson-677967b8b7" duration="33.484µs"
I1121 21:06:11.006000       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-world-robson-677967b8b7" duration="6.671559ms"
I1121 21:06:11.006132       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-world-robson-677967b8b7" duration="48.012µs"
I1121 21:06:14.088383       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [91caf91f77ac] <==
E1121 20:57:49.263357       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1121 20:57:49.271753       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1121 20:57:49.281515       1 server_linux.go:66] "Using iptables proxy"
I1121 20:57:49.648311       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1121 20:57:49.648443       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1121 20:57:49.670346       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1121 20:57:49.670652       1 server_linux.go:169] "Using iptables Proxier"
I1121 20:57:49.673754       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1121 20:57:49.680642       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1121 20:57:49.688138       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1121 20:57:49.688334       1 server.go:483] "Version info" version="v1.31.0"
I1121 20:57:49.688387       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1121 20:57:49.689583       1 config.go:326] "Starting node config controller"
I1121 20:57:49.689627       1 shared_informer.go:313] Waiting for caches to sync for node config
I1121 20:57:49.689698       1 config.go:104] "Starting endpoint slice config controller"
I1121 20:57:49.689708       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1121 20:57:49.690360       1 config.go:197] "Starting service config controller"
I1121 20:57:49.691943       1 shared_informer.go:313] Waiting for caches to sync for service config
I1121 20:57:49.790381       1 shared_informer.go:320] Caches are synced for node config
I1121 20:57:49.790431       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1121 20:57:49.792730       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [0847ca281ec8] <==
I1121 20:57:40.929851       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1121 20:57:40.929907       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1121 20:57:40.930186       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W1121 20:57:40.934024       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1121 20:57:40.934072       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1121 20:57:40.934127       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1121 20:57:40.934127       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1121 20:57:40.934168       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1121 20:57:40.934179       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1121 20:57:40.934192       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1121 20:57:40.934199       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1121 20:57:40.934267       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1121 20:57:40.934292       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1121 20:57:40.934299       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1121 20:57:40.934312       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1121 20:57:40.934318       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1121 20:57:40.934335       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1121 20:57:40.934363       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1121 20:57:40.934393       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1121 20:57:40.934459       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1121 20:57:40.934490       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1121 20:57:40.934397       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1121 20:57:40.934508       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1121 20:57:40.934516       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1121 20:57:40.934072       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1121 20:57:40.934334       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1121 20:57:40.934533       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1121 20:57:40.934544       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1121 20:57:40.934558       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1121 20:57:40.934415       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1121 20:57:40.934635       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1121 20:57:40.934595       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1121 20:57:40.934661       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1121 20:57:41.820015       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1121 20:57:41.820074       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1121 20:57:41.934923       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1121 20:57:41.934987       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1121 20:57:42.007918       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1121 20:57:42.007975       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1121 20:57:42.032945       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1121 20:57:42.033019       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1121 20:57:42.045727       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1121 20:57:42.045780       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1121 20:57:42.072470       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1121 20:57:42.072543       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1121 20:57:42.080124       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1121 20:57:42.080211       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1121 20:57:42.101962       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1121 20:57:42.102035       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1121 20:57:42.134390       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1121 20:57:42.134451       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1121 20:57:42.138820       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1121 20:57:42.138878       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1121 20:57:42.139953       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1121 20:57:42.140012       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1121 20:57:42.151733       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1121 20:57:42.151810       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1121 20:57:42.471307       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1121 20:57:42.471395       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I1121 20:57:44.733766       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Nov 21 21:05:59 minikube kubelet[2491]: I1121 21:05:59.037579    2491 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-x4ht8\" (UniqueName: \"kubernetes.io/projected/8aaba8db-c768-4294-b38c-48858444217e-kube-api-access-x4ht8\") pod \"hello-world-robson-677967b8b7-wn2b6\" (UID: \"8aaba8db-c768-4294-b38c-48858444217e\") " pod="default/hello-world-robson-677967b8b7-wn2b6"
Nov 21 21:05:59 minikube kubelet[2491]: I1121 21:05:59.138273    2491 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8ltfl\" (UniqueName: \"kubernetes.io/projected/c6614c7c-fe12-46b8-b99c-a19d91ab7009-kube-api-access-8ltfl\") pod \"hello-world-robson-677967b8b7-wn9x4\" (UID: \"c6614c7c-fe12-46b8-b99c-a19d91ab7009\") " pod="default/hello-world-robson-677967b8b7-wn9x4"
Nov 21 21:06:05 minikube kubelet[2491]: I1121 21:06:05.700661    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:06:05 minikube kubelet[2491]: E1121 21:06:05.707508    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:06:05 minikube kubelet[2491]: E1121 21:06:05.707683    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:06:05 minikube kubelet[2491]: E1121 21:06:05.708937    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:06:09 minikube kubelet[2491]: I1121 21:06:09.978147    2491 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/hello-world-robson-677967b8b7-wn2b6" podStartSLOduration=2.293723989 podStartE2EDuration="11.978125542s" podCreationTimestamp="2024-11-21 21:05:58 +0000 UTC" firstStartedPulling="2024-11-21 21:05:59.60499201 +0000 UTC m=+496.051839463" lastFinishedPulling="2024-11-21 21:06:09.290373602 +0000 UTC m=+505.736241016" observedRunningTime="2024-11-21 21:06:09.977752511 +0000 UTC m=+506.423619935" watchObservedRunningTime="2024-11-21 21:06:09.978125542 +0000 UTC m=+506.423992956"
Nov 21 21:06:10 minikube kubelet[2491]: I1121 21:06:10.999389    2491 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/hello-world-robson-677967b8b7-wn9x4" podStartSLOduration=1.983183436 podStartE2EDuration="12.999362141s" podCreationTimestamp="2024-11-21 21:05:58 +0000 UTC" firstStartedPulling="2024-11-21 21:05:59.633774326 +0000 UTC m=+496.080621769" lastFinishedPulling="2024-11-21 21:06:10.65093306 +0000 UTC m=+507.096800474" observedRunningTime="2024-11-21 21:06:10.999349907 +0000 UTC m=+507.445217321" watchObservedRunningTime="2024-11-21 21:06:10.999362141 +0000 UTC m=+507.445229575"
Nov 21 21:06:19 minikube kubelet[2491]: I1121 21:06:19.692783    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:06:19 minikube kubelet[2491]: E1121 21:06:19.700788    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:06:19 minikube kubelet[2491]: E1121 21:06:19.700942    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:06:19 minikube kubelet[2491]: E1121 21:06:19.702194    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:06:30 minikube kubelet[2491]: I1121 21:06:30.702070    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:06:30 minikube kubelet[2491]: E1121 21:06:30.709841    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:06:30 minikube kubelet[2491]: E1121 21:06:30.710018    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:06:30 minikube kubelet[2491]: E1121 21:06:30.711369    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:06:42 minikube kubelet[2491]: I1121 21:06:42.700775    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:06:42 minikube kubelet[2491]: E1121 21:06:42.707639    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:06:42 minikube kubelet[2491]: E1121 21:06:42.707805    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:06:42 minikube kubelet[2491]: E1121 21:06:42.709012    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:06:54 minikube kubelet[2491]: I1121 21:06:54.702374    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:06:54 minikube kubelet[2491]: E1121 21:06:54.709160    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:06:54 minikube kubelet[2491]: E1121 21:06:54.709388    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:06:54 minikube kubelet[2491]: E1121 21:06:54.710702    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:07:40 minikube kubelet[2491]: I1121 21:07:40.530109    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:07:40 minikube kubelet[2491]: E1121 21:07:40.536722    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:07:40 minikube kubelet[2491]: E1121 21:07:40.536873    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:07:40 minikube kubelet[2491]: E1121 21:07:40.538428    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:07:55 minikube kubelet[2491]: I1121 21:07:55.530908    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:07:55 minikube kubelet[2491]: E1121 21:07:55.538960    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:07:55 minikube kubelet[2491]: E1121 21:07:55.539144    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:07:55 minikube kubelet[2491]: E1121 21:07:55.540514    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:07:36 minikube kubelet[2491]: I1121 21:07:36.701334    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:07:36 minikube kubelet[2491]: E1121 21:07:36.709563    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:07:36 minikube kubelet[2491]: E1121 21:07:36.709929    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:07:36 minikube kubelet[2491]: E1121 21:07:36.711374    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:07:51 minikube kubelet[2491]: I1121 21:07:51.700742    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:07:51 minikube kubelet[2491]: E1121 21:07:51.708345    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:07:51 minikube kubelet[2491]: E1121 21:07:51.708503    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:07:51 minikube kubelet[2491]: E1121 21:07:51.709751    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:08:04 minikube kubelet[2491]: I1121 21:08:04.700692    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:08:04 minikube kubelet[2491]: E1121 21:08:04.707574    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:08:04 minikube kubelet[2491]: E1121 21:08:04.707719    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:08:04 minikube kubelet[2491]: E1121 21:08:04.708978    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:08:19 minikube kubelet[2491]: I1121 21:08:19.698861    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:08:19 minikube kubelet[2491]: E1121 21:08:19.707071    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:08:19 minikube kubelet[2491]: E1121 21:08:19.707200    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:08:19 minikube kubelet[2491]: E1121 21:08:19.708480    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:08:30 minikube kubelet[2491]: I1121 21:08:30.699771    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:08:30 minikube kubelet[2491]: E1121 21:08:30.706678    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:08:30 minikube kubelet[2491]: E1121 21:08:30.706826    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:08:30 minikube kubelet[2491]: E1121 21:08:30.708082    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:08:41 minikube kubelet[2491]: I1121 21:08:41.698519    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:08:41 minikube kubelet[2491]: E1121 21:08:41.705427    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:08:41 minikube kubelet[2491]: E1121 21:08:41.705578    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:08:41 minikube kubelet[2491]: E1121 21:08:41.706862    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"
Nov 21 21:08:55 minikube kubelet[2491]: I1121 21:08:55.698035    2491 scope.go:117] "RemoveContainer" containerID="64cb52c29a0f3d06b55f608aef71a029abd02e7e1af7ac7f4c22f48e8bf8fd99"
Nov 21 21:08:55 minikube kubelet[2491]: E1121 21:08:55.704812    2491 log.go:32] "CreateContainer in sandbox from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." podSandboxID="670adcc699129d8e0979621a54ac60708848bd56f35c9599a606e652420452de"
Nov 21 21:08:55 minikube kubelet[2491]: E1121 21:08:55.704962    2491 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-csdfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod storage-provisioner_kube-system(e7b029f0-83e5-4e8a-b314-fc4cfda721eb): CreateContainerError: Error response from daemon: Conflict. The container name \"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\" is already in use by container \"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\". You have to remove (or rename) that container to be able to reuse that name." logger="UnhandledError"
Nov 21 21:08:55 minikube kubelet[2491]: E1121 21:08:55.706242    2491 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerError: \"Error response from daemon: Conflict. The container name \\\"/k8s_storage-provisioner_storage-provisioner_kube-system_e7b029f0-83e5-4e8a-b314-fc4cfda721eb_1\\\" is already in use by container \\\"c85884823201a24932d1271212c37b3622779cb99140f4bf8f1e0cf4be57440b\\\". You have to remove (or rename) that container to be able to reuse that name.\"" pod="kube-system/storage-provisioner" podUID="e7b029f0-83e5-4e8a-b314-fc4cfda721eb"


==> storage-provisioner [64cb52c29a0f] <==
I1121 20:57:48.189999       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1121 20:58:49.877332       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [c85884823201] <==
I1121 20:58:18.386554       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1121 20:58:18.394747       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1121 20:58:18.394839       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1121 20:58:18.406591       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1121 20:58:18.406734       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"571bab71-8391-40f9-b126-ab6b34c517dd", APIVersion:"v1", ResourceVersion:"423", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_2cd73fc0-0d67-4880-b793-bed87110364f became leader
I1121 20:58:18.406848       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_2cd73fc0-0d67-4880-b793-bed87110364f!
I1121 20:58:18.507258       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_2cd73fc0-0d67-4880-b793-bed87110364f!

